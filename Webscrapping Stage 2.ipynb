{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fc6abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cloudscraper in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (1.2.69)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from cloudscraper) (3.0.9)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from cloudscraper) (0.10.1)\n",
      "Requirement already satisfied: requests>=2.9.2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from cloudscraper) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\anaconda\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\program files\\anaconda\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.9.2->cloudscraper) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\anaconda\\lib\\site-packages (from requests>=2.9.2->cloudscraper) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import cloudscraper\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69742501",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_file_1 = pd.read_csv('links_file_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde1dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    title_and_views = soup.find('h2')\n",
    "    if title_and_views is None:\n",
    "        return 'PLACEHOLDER', 'PLACEHOLDER'\n",
    "    title_parts = title_and_views.text.strip().split(' - ')\n",
    "    return title_parts[0], title_parts[-2] if len(title_parts) >= 2 else 'PLACEHOLDER'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f1f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(soup):\n",
    "    body = soup.find('div', {'class': 'narrow'})\n",
    "    return body.text.strip() if body is not None else 'PLACEHOLDER'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595ed93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_and_count(soup, scraper, url):\n",
    "    all_comments = []\n",
    "    previous_comments = None\n",
    "    page = 0\n",
    "    while True:\n",
    "        current_url = f\"{url}/{page}\" if page > 0 else url\n",
    "        raw_html = scraper.get(current_url).text\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "        comments = []\n",
    "        for div in soup.find_all('div', class_='narrow'):\n",
    "            if div.blockquote:\n",
    "                div.blockquote.extract()\n",
    "            comments.append(div.text.strip())\n",
    "\n",
    "        if comments == previous_comments:\n",
    "            break\n",
    "\n",
    "        all_comments.extend(comments)\n",
    "        previous_comments = comments\n",
    "        page += 1\n",
    "\n",
    "    comments_joined = ';;;'.join(all_comments)\n",
    "    comment_count = len(all_comments)\n",
    "    return comments_joined, comment_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9952429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_views(soup):\n",
    "    views_element = soup.find('p', class_='bold')\n",
    "    if views_element:\n",
    "        views_text = views_element.text.strip()\n",
    "        views_match = re.search(r'\\((\\d{1,3}(,\\d{3})*|\\d+)\\s*Views\\)', views_text)\n",
    "        if views_match:\n",
    "            return int(views_match.group(1).replace(',', ''))\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f34726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename and the header of the CSV file\n",
    "filename = 'Nairaland Dataset Part1.csv'\n",
    "header = ['Title', 'Body', 'Category', 'Comments', 'Comment Count', 'Time Posted', 'Date Posted', 'Total Views', 'Url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ddc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing for 10 seconds after 3 iterations\n"
     ]
    }
   ],
   "source": [
    "# Loop through the list of URLs and write the data to the CSV file\n",
    "with open(filename, mode='a', encoding='utf-8', newline='') as file:\n",
    "   # writer = csv.writer(file)\n",
    "    writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "  \n",
    "    writer.writerow(header)\n",
    "    counter = 0  # Counts the number of iterations\n",
    "\n",
    "    for index, row in links_file_1.iterrows():\n",
    "        url = row['Url']\n",
    "        post_time = row['Time posted']\n",
    "        post_date = row['Date posted']\n",
    "        \n",
    "        scraper = cloudscraper.create_scraper()\n",
    "        \n",
    "        try:  # Add try block here\n",
    "            raw_text = scraper.get(url).text\n",
    "            soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "\n",
    "            Title, Category = get_title(soup)\n",
    "            Body = get_body(soup)\n",
    "            comments_joined, comment_count = get_comments_and_count(soup, scraper, url)\n",
    "            total_views = get_total_views(soup)\n",
    "\n",
    "            data = [Title, Body, Category, comments_joined, comment_count, post_time, post_date, total_views, url]\n",
    "            writer.writerow(data)\n",
    "            \n",
    "        \n",
    "        except Exception as e:  # Add except block here\n",
    "            print(f\"Error occurred while processing {url}: {e}\")\n",
    "\n",
    "\n",
    "        # Add delay after every 3 iterations to avoid overloading the server\n",
    "        if counter % 3 == 0 and counter != 0:\n",
    "            print(f\"Pausing for 10 seconds after {counter} iterations\")\n",
    "            time.sleep(10)\n",
    "        counter += 1\n",
    "        \n",
    "                                                                                                                             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
